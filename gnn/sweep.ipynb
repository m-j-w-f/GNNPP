{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xL1uIysaxXeo"
   },
   "source": [
    "# Sweep\n",
    "\n",
    "This Notebook is used to run a hyperparameter search using a sweep from W&B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    " - Write this as a python only function and use if __name__ == \"__main__\":\n",
    "    train()\n",
    " - Check assertion error for Conv\n",
    " - Run from command line then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Notebook Name for WandB\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'sweep.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T22:32:46.883841Z",
     "start_time": "2023-05-25T22:32:46.222127Z"
    },
    "id": "wNM6n3pzxXes"
   },
   "outputs": [],
   "source": [
    "from helpers import load_data, load_stations, clean_data, normalize_data, create_data, visualize_graph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv, Sequential, summary\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear, Embedding, Dropout, ModuleList\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopy.distance\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import traceback\n",
    "import wandb\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T22:33:25.274277Z",
     "start_time": "2023-05-25T22:33:17.750095Z"
    },
    "id": "a4YlQ6WXxXet"
   },
   "outputs": [],
   "source": [
    "# Get Data from feather\n",
    "data = load_data(indexed=False)\n",
    "# Get List of stations with all stations -> will break further code if cut already\n",
    "stations = load_stations(data)\n",
    "# Clean Data\n",
    "data = clean_data(data, max_missing=121, max_alt=1000.0)\n",
    "# Normalize Data\n",
    "normalized_data = normalize_data(data, last_obs=-365) #last_obs is -365 since the last year is used for testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T18:03:42.033094Z",
     "start_time": "2023-05-23T18:03:38.328937Z"
    },
    "id": "Usqz-8knxXet"
   },
   "source": [
    "## Create the torch dataset\n",
    "\n",
    "The Dataset which is a `pandas.DataFrame` gets converted to a `torch_geometric.data` object, which then can be processed by the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dist_matrix = compute_dist_matrix(stations)\n",
    "#np.save(dist_matrix, 'dist_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T22:40:01.945004Z",
     "start_time": "2023-05-25T22:39:13.566162Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40648,
     "status": "ok",
     "timestamp": 1683747193612,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "VjSzAVBUxXeu",
    "outputId": "1c8425f2-2768-4f78-aec9-b2e5a942bac9"
   },
   "outputs": [],
   "source": [
    "# TODO set the maximum distance (might want to get this from a configuration file)\n",
    "def build_dataloaders(max_dist: int, batch_size: int):\n",
    "    dist_matrix = np.load('dist_matrix.npy')\n",
    "\n",
    "    # Create a boolean mask indicating which edges to include\n",
    "    mask = (dist_matrix <= max_dist) & (dist_matrix != 0)\n",
    "\n",
    "    torch_data = []\n",
    "    for date in tqdm(data['date'].unique(), desc=\"Building dataset\"):\n",
    "        torch_data.append(create_data(df=normalized_data, date=date, mask=mask, dist_matrix=dist_matrix))\n",
    "\n",
    "    # Definition of train_loader and valid_loader\n",
    "    train_loader = DataLoader(torch_data[:-730], batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(torch_data[-730:-365], batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(torch_data[-365:], batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T18:35:08.850757Z",
     "start_time": "2023-05-23T18:35:08.141967Z"
    },
    "id": "717jQLucxXey"
   },
   "source": [
    "## GNN\n",
    "\n",
    "In the following, some layers for the GNN and the loss function are defined."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T18:05:45.443622Z",
     "start_time": "2023-05-23T18:05:44.674331Z"
    }
   },
   "source": [
    "### CRPS Loss Function\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "    \\operatorname{crps}(F,X)=&\\int_{-\\inf}^{\\inf}\\left(F(y)-\\boldsymbol{1}_{(y-x)}\\right)^2dy\\\\\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "Closed form expression from Gneiting et al. (2005)\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "    \\operatorname{crps}\\left(\\mathcal{N}\\left(\\mu, \\sigma^2\\right), y\\right)= & \\sigma\\left\\{\\frac{y-\\mu}{\\sigma}\\left[2 \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right)-1\\right] +2 \\varphi\\left(\\frac{y-\\mu}{\\sigma}\\right)-\\frac{1}{\\sqrt{\\pi}}\\right\\}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "$\\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right)$ denotes the CDF of a standard normal distribution and $\\varphi\\left(\\frac{y-\\mu}{\\sigma}\\right)$ denotes the PDF of a standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T21:57:49.882654Z",
     "start_time": "2023-05-25T21:57:49.878788Z"
    }
   },
   "outputs": [],
   "source": [
    "def crps(mu: torch.tensor, sigma: torch.tensor, y: torch.tensor):\n",
    "    \"\"\"Calculates the Continuous Ranked Probability Score (CRPS) assuming normally distributed df\n",
    "\n",
    "    Args:\n",
    "        mu (torch.tensor): mean\n",
    "        sigma (torch.tensor): standard deviation\n",
    "        y (torch.tensor): observed df\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: CRPS value\n",
    "    \"\"\"\n",
    "    y = y.view((-1,1)) # make sure y has the right shape\n",
    "    PI = np.pi #3.14159265359\n",
    "    omega = (y - mu) / sigma\n",
    "    # PDF of normal distribution at omega\n",
    "    pdf = 1/(torch.sqrt(torch.tensor(2 * PI))) * torch.exp(-0.5 * omega ** 2)\n",
    "    \n",
    "    # Source: https://stats.stackexchange.com/questions/187828/how-are-the-error-function-and-standard-normal-distribution-function-related\n",
    "    cdf = 0.5 * (1 + torch.erf(omega / torch.sqrt(torch.tensor(2))))\n",
    "    \n",
    "    crps = sigma * (omega * (2 * cdf - 1) + 2 * pdf - 1/torch.sqrt(torch.tensor(PI)))\n",
    "    return  torch.mean(crps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN\n",
    "Definition of Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T21:59:15.635664Z",
     "start_time": "2023-05-25T21:59:15.595417Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1683747211276,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "9rNDdBPQxXey",
    "outputId": "0e0462f5-6195-4e9f-acf8-aaaf65cb9597"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T21:57:58.827984Z",
     "start_time": "2023-05-25T21:57:58.811673Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Convolution(torch.nn.Module):\n",
    "    def __init__(self, out_channels, hidden_channels, heads, num_layers:int=None):\n",
    "        super(Convolution, self).__init__()\n",
    "        # Make sure either hidden_channels is a list, heads is a list or num_layer is supplied\n",
    "        assert isinstance(hidden_channels, list) or isinstance(heads, list) or num_layers is not None,\\\n",
    "                \"If hidden_channels is not a list, num_layers must be specified.\"\n",
    "        # both are a list\n",
    "        if isinstance(hidden_channels, list) and isinstance(heads, list):\n",
    "            assert len(hidden_channels) == len(heads), f\"Lengths of lists {len(hidden_channels)} and {len(heads)} do not match.\"\n",
    "        # only hidden_channels is list\n",
    "        if isinstance(hidden_channels, list) and not isinstance(heads, list):\n",
    "            heads = [heads] * len(hidden_channels)\n",
    "        # only heads is list\n",
    "        if isinstance(heads, list) and not isinstance(hidden_channels, list):\n",
    "            hidden_channels = [hidden_channels] * len(heads)\n",
    "        # none is list\n",
    "        if not isinstance(heads, list) and not isinstance(hidden_channels, list):\n",
    "            heads = [heads] * num_layers\n",
    "            hidden_channels = [hidden_channels] * num_layers\n",
    "        \n",
    "        # definition of Layers\n",
    "        self.convolutions = ModuleList()\n",
    "        for c, h in zip(hidden_channels, heads):\n",
    "            self.convolutions.append(GATv2Conv(in_channels=-1, out_channels=c, heads=h, edge_dim=1))\n",
    "        # Last Layer to match shape of output\n",
    "        self.lin = Linear(in_features=hidden_channels[-1] * heads[-1], out_features=out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        \n",
    "        x = F.relu(self.lin(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbedStations(torch.nn.Module):\n",
    "    def __init__(self, num_stations_max, embedding_dim):\n",
    "        super(EmbedStations, self).__init__()\n",
    "        self.embed = Embedding(num_embeddings=num_stations_max, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        station_ids = x[:, 0].long()\n",
    "        emb_station = self.embed(station_ids)\n",
    "        x = torch.cat((emb_station, x[:, 1:]), dim=1) # Concatenate embedded station_id to rest of the feature vector\n",
    "        return x\n",
    "\n",
    "\n",
    "class MakePositive(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MakePositive, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = torch.split(x, 1, dim=-1)\n",
    "        sigma = F.softplus(sigma) # ensure that sigma is positive\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "class ResGnn(torch.nn.Module):\n",
    "    def __init__(self, out_channels, num_layers, hidden_channels, heads):\n",
    "        super(ResGnn, self).__init__()\n",
    "        assert num_layers > 0, \"num_layers must be > 0.\"\n",
    "\n",
    "        # Create Layers\n",
    "        self.convolutions = ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.convolutions.append(GATv2Conv(-1, hidden_channels, heads=heads, edge_dim=1))\n",
    "            else:\n",
    "                self.convolutions.append((GATv2Conv(-1, hidden_channels, heads=heads, edge_dim=1)))\n",
    "        self.lin = Linear(hidden_channels * heads, out_channels) #hier direkt 2 testen\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "        for i, conv in enumerate(self.convolutions):\n",
    "            if i == 0:\n",
    "                # First Layer\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "                x = F.relu(x)\n",
    "            else:\n",
    "                x = x + F.relu(conv(x, edge_index, edge_attr)) # Residual Layers\n",
    "\n",
    "        x = self.lin(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embed_dim:int, hidden_channels:int, heads:int, num_layers:int, linear_size:int, type:str):\n",
    "    \"\"\"Builds  a model with the specified parameters\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): embedding dimension of the station id\n",
    "        hidden_channels (int): number of hidden channels used by the convolution layers\n",
    "        heads (int): number of heads used for the attention of the convolution layers\n",
    "        num_layers (int): depth of the convolution layers\n",
    "        linear_size (int): size of the linear layer\n",
    "        type (str): type of the model, either 'ResGNNv2' or 'GATConvv2'\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        _type_: returns a model with the specified parameters\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if type == 'ResGNNv2':\n",
    "        conv = (ResGnn(out_channels=linear_size, hidden_channels=hidden_channels, heads=heads, num_layers=num_layers), 'x, edge_index, edge_attr -> x')\n",
    "    elif type == 'GATConvv2':\n",
    "        conv = (Convolution(out_channels=linear_size, hidden_channels=hidden_channels, heads=heads, num_layers=num_layers), 'x, edge_index, edge_attr -> x')\n",
    "    \n",
    "    model = Sequential('x, edge_index, edge_attr',\n",
    "                   [\n",
    "                       (EmbedStations(num_stations_max=535, embedding_dim=embed_dim), 'x -> x'),\n",
    "                       conv,\n",
    "                       (Linear(linear_size, 2),'x -> x'),\n",
    "                       (MakePositive(), 'x -> mu, sigma')\n",
    "                   ])\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_optimizer(model, learning_rate: float) -> torch.optim.Optimizer:\n",
    "    \"\"\"Defines the optimizer for the model\n",
    "\n",
    "\n",
    "    Args:\n",
    "        model (_type_): model for which the optimizer is defined\n",
    "        learning_rate (float): learning rate\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Optimizer: returns the optimizer for the model\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader):\n",
    "    model.eval()\n",
    "    mu_list = []\n",
    "    sigma_list = []\n",
    "    err_list = []\n",
    "    y_list = []\n",
    "\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch.to(device)\n",
    "        mu, sigma = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        y = batch.y\n",
    "        err = crps(mu, sigma, y)\n",
    "        mu = mu.detach().cpu().numpy().flatten()\n",
    "        sigma = sigma.detach().cpu().numpy().flatten()\n",
    "        y = y.cpu().numpy()\n",
    "        err = err.detach().cpu().numpy()\n",
    "\n",
    "        mu_list.append(mu)\n",
    "        sigma_list.append(sigma)\n",
    "        y_list.append(y)\n",
    "        err_list.append(err*len(batch))\n",
    "\n",
    "    err = sum(err_list) / len(test_loader.dataset)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        train_loader, valid_loader, test_loader = build_dataloaders(max_dist=config.max_dist, batch_size=config.batch_size)\n",
    "        \n",
    "        model = build_model(embed_dim=config.embed_dim,\n",
    "                            hidden_channels=config.hidden_channels,\n",
    "                            heads=config.heads,\n",
    "                            num_layers=config.num_layers,\n",
    "                            linear_size=config.linear_size,\n",
    "                            type=config.type)\n",
    "        \n",
    "        optimizer = build_optimizer(model=model, learning_rate=config.learning_rate)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        def train(batch):\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            mu, sigma = out\n",
    "            loss = crps(mu, sigma, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return loss\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def valid(batch):\n",
    "            batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            mu, sigma = out\n",
    "            loss = crps(mu, sigma, batch.y)\n",
    "            return loss\n",
    "        \n",
    "        epochs_pbar = trange(config.max_epochs, desc=\"Epochs\")\n",
    "        for epoch in epochs_pbar:\n",
    "            # Train for one epoch\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                loss = train(batch)\n",
    "                train_loss += loss.item() * batch.num_graphs\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "                \n",
    "            # Evaluate on the validation set\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for batch in valid_loader:\n",
    "                loss = valid(batch)\n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "            val_loss /= len(valid_loader.dataset)\n",
    "            \n",
    "            # Check if the validation loss has improved\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "                # Save model checkpoint\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, \"checkpoint.pt\")\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "            \n",
    "            # Log to WandB\n",
    "            wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss})\n",
    "            epochs_pbar.set_postfix({\"Train Loss\": train_loss, \"Val Loss\": val_loss, \"Best Loss\": best_val_loss, \"No Improvement\": no_improvement})\n",
    "            # Early stopping\n",
    "            if no_improvement == config.patience:\n",
    "                print('Early stopping.')\n",
    "                break\n",
    "        \n",
    "        # Load weights from model checkpoint\n",
    "        checkpoint = torch.load(\"checkpoint.pt\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        test_error = eval(model=model, test_loader=test_loader)\n",
    "        \n",
    "        wandb.log({\"best_val_loss\": best_val_loss,\n",
    "                   \"trained_epochs\": epoch-config.patience,\n",
    "                   \"evaluation_error\": test_error})\n",
    "        \n",
    "        # Free memory\n",
    "        model.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "def train_model_catch_errors(config=None):\n",
    "    try:\n",
    "        train_model(config=config)\n",
    "    except Exception as e:\n",
    "        # exit gracefully, so wandb logs the problem\n",
    "        print(traceback.print_exc())\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2z1tkl5c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \theads: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_channels: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002769256134796316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlinear_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_dist: 131\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 250\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttype: GATConvv2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfeik\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\morit\\GNNPP\\gnn\\wandb\\run-20230602_120828-2z1tkl5c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/feik/GNNPP/runs/2z1tkl5c' target=\"_blank\">misty-sweep-13</a></strong> to <a href='https://wandb.ai/feik/GNNPP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/feik/GNNPP/sweeps/ja2lwn8o' target=\"_blank\">https://wandb.ai/feik/GNNPP/sweeps/ja2lwn8o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/feik/GNNPP' target=\"_blank\">https://wandb.ai/feik/GNNPP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/feik/GNNPP/sweeps/ja2lwn8o' target=\"_blank\">https://wandb.ai/feik/GNNPP/sweeps/ja2lwn8o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/feik/GNNPP/runs/2z1tkl5c' target=\"_blank\">https://wandb.ai/feik/GNNPP/runs/2z1tkl5c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset: 100%|██████████| 3530/3530 [00:31<00:00, 110.42it/s]\n",
      "Epochs:   6%|▌         | 15/250 [06:06<1:34:27, 24.12s/it, Train Loss=0.878, Val Loss=0.915, Best Loss=0.915, No Improvement=0]"
     ]
    }
   ],
   "source": [
    "wandb.agent(\"feik/GNNPP/ja2lwn8o\", train_model_catch_errors, count=100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/m-j-w-f/GNNPP/blob/testing/gnn/gnn_test.ipynb",
     "timestamp": 1683748469885
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "GNNPP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
