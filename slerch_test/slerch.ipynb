{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNPP\n",
    "This Notebook reimplements the Method of the Paper from RL 18 to compare results to the pytoch geometric model.\n",
    "This should archieve a CRPS score around 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data (can be downloaded from https://doi.org/10.6084/m9.figshare.13516301.v1)\n",
    "\n",
    "data = pd.read_feather(path = \"C:/Users/morit/GNNPP/data_RL18.feather\")\n",
    "data.station = pd.to_numeric(data.station, downcast = 'integer')\n",
    "\n",
    "# drop soil moisture predictions due to missing values\n",
    "# Note that this is a minor change compared to the paper, but does not have a significant effect\n",
    "data = data.drop(['sm_mean', 'sm_var'], axis=1)\n",
    "\n",
    "# split into train and test data\n",
    "eval_start = 1626724\n",
    "train_end = 1626723\n",
    "\n",
    "train_features_raw = data.iloc[:train_end,3:42].to_numpy()\n",
    "train_targets = data.iloc[:train_end,2].to_numpy()\n",
    "train_IDs = data.iloc[:train_end,1].to_numpy()\n",
    "\n",
    "test_features_raw = data.iloc[eval_start:,3:42].to_numpy()\n",
    "test_targets = data.iloc[eval_start:,2].to_numpy()\n",
    "test_IDs = data.iloc[eval_start:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "def normalize(data, method=None, shift=None, scale=None):\n",
    "    result = np.zeros(data.shape)\n",
    "    if method == \"MAX\":\n",
    "        scale = np.max(data, axis=0)\n",
    "        shift = np.zeros(scale.shape)\n",
    "    for index in range(len(data[0])):\n",
    "        result[:,index] = (data[:,index] - shift[index]) / scale[index]\n",
    "    return result, shift, scale\n",
    "\n",
    "train_features, train_shift, train_scale = normalize(train_features_raw[:,:], method=\"MAX\")\n",
    "\n",
    "test_features = normalize(test_features_raw[:,:], shift=train_shift, scale=train_scale)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps(mu: torch.tensor, sigma: torch.tensor, y: torch.tensor):\n",
    "    \"\"\"Calculates the Continuous Ranked Probability Score (CRPS) assuming normally distributed df\n",
    "\n",
    "    Args:\n",
    "        mu (torch.tensor): mean\n",
    "        sigma (torch.tensor): standard deviation\n",
    "        y (torch.tensor): observed df\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: CRPS value\n",
    "    \"\"\"\n",
    "    y = y.view((-1,1)) # make sure y has the right shape\n",
    "    PI = np.pi #3.14159265359\n",
    "    omega = (y - mu) / sigma\n",
    "    # PDF of normal distribution at omega\n",
    "    pdf = 1/(torch.sqrt(torch.tensor(2 * PI))) * torch.exp(-0.5 * omega ** 2)\n",
    "    \n",
    "    # Source: https://stats.stackexchange.com/questions/187828/how-are-the-error-function-and-standard-normal-distribution-function-related\n",
    "    cdf = 0.5 * (1 + torch.erf(omega / torch.sqrt(torch.tensor(2))))\n",
    "    \n",
    "    crps = sigma * (omega * (2 * cdf - 1) + 2 * pdf - 1/torch.sqrt(torch.tensor(PI)))\n",
    "    return  torch.mean(crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class NNPP(nn.Module):\n",
    "    def __init__(self, INPUT_DIM:int):\n",
    "        super(NNPP, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=535, embedding_dim=2)\n",
    "        self.lin1 = nn.Linear(in_features=INPUT_DIM+2-1, out_features=512)\n",
    "        self.lin2 = nn.Linear(in_features=512, out_features=2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        station_ids = x[:, 0].long()\n",
    "        emb_station = self.emb(station_ids)\n",
    "        x = torch.cat((emb_station, x[:, 1:]), dim=1) # Concatenate embedded station_id to rest of the feature vector\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        mu, sigma = torch.split(x, 1, dim=-1)\n",
    "        sigma = F.softplus(sigma) # ensure that sigma is positive\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Training...\n",
      "Epoch: 1/100, Loss: 1.9579\n",
      "Epoch: 2/100, Loss: 1.3119\n",
      "Epoch: 3/100, Loss: 1.2443\n",
      "Epoch: 4/100, Loss: 1.2005\n",
      "Epoch: 5/100, Loss: 1.1623\n",
      "Epoch: 6/100, Loss: 1.1327\n",
      "Epoch: 7/100, Loss: 1.1071\n",
      "Epoch: 8/100, Loss: 1.0870\n",
      "Epoch: 9/100, Loss: 1.0728\n",
      "Epoch: 10/100, Loss: 1.0593\n",
      "Epoch: 11/100, Loss: 1.0488\n",
      "Epoch: 12/100, Loss: 1.0388\n",
      "Epoch: 13/100, Loss: 1.0315\n",
      "Epoch: 14/100, Loss: 1.0235\n",
      "Epoch: 15/100, Loss: 1.0169\n",
      "Epoch: 16/100, Loss: 1.0130\n",
      "Epoch: 17/100, Loss: 1.0059\n",
      "Epoch: 18/100, Loss: 1.0027\n",
      "Epoch: 19/100, Loss: 0.9981\n",
      "Epoch: 20/100, Loss: 0.9953\n",
      "Epoch: 21/100, Loss: 0.9900\n",
      "Epoch: 22/100, Loss: 0.9867\n",
      "Epoch: 23/100, Loss: 0.9851\n",
      "Epoch: 24/100, Loss: 0.9806\n",
      "Epoch: 25/100, Loss: 0.9799\n",
      "Epoch: 26/100, Loss: 0.9761\n",
      "Epoch: 27/100, Loss: 0.9718\n",
      "Epoch: 28/100, Loss: 0.9720\n",
      "Epoch: 29/100, Loss: 0.9675\n",
      "Epoch: 30/100, Loss: 0.9675\n",
      "Epoch: 31/100, Loss: 0.9634\n",
      "Epoch: 32/100, Loss: 0.9630\n",
      "Epoch: 33/100, Loss: 0.9630\n",
      "Epoch: 34/100, Loss: 0.9568\n",
      "Epoch: 35/100, Loss: 0.9599\n",
      "Epoch: 36/100, Loss: 0.9585\n",
      "Epoch: 37/100, Loss: 0.9528\n",
      "Epoch: 38/100, Loss: 0.9546\n",
      "Epoch: 39/100, Loss: 0.9520\n",
      "Epoch: 40/100, Loss: 0.9494\n",
      "Epoch: 41/100, Loss: 0.9495\n",
      "Epoch: 42/100, Loss: 0.9471\n",
      "Epoch: 43/100, Loss: 0.9495\n",
      "Epoch: 44/100, Loss: 0.9452\n",
      "Epoch: 45/100, Loss: 0.9449\n",
      "Epoch: 46/100, Loss: 0.9434\n",
      "Epoch: 47/100, Loss: 0.9422\n",
      "Epoch: 48/100, Loss: 0.9408\n",
      "Epoch: 49/100, Loss: 0.9433\n",
      "Epoch: 50/100, Loss: 0.9431\n",
      "Epoch: 51/100, Loss: 0.9391\n",
      "Epoch: 52/100, Loss: 0.9382\n",
      "Epoch: 53/100, Loss: 0.9368\n",
      "Epoch: 54/100, Loss: 0.9409\n",
      "Epoch: 55/100, Loss: 0.9338\n",
      "Epoch: 56/100, Loss: 0.9354\n",
      "Epoch: 57/100, Loss: 0.9359\n",
      "Epoch: 58/100, Loss: 0.9354\n",
      "Epoch: 59/100, Loss: 0.9341\n",
      "Epoch: 60/100, Loss: 0.9304\n",
      "Epoch: 61/100, Loss: 0.9338\n",
      "Epoch: 62/100, Loss: 0.9325\n",
      "Epoch: 63/100, Loss: 0.9308\n",
      "Epoch: 64/100, Loss: 0.9311\n",
      "Epoch: 65/100, Loss: 0.9274\n",
      "Epoch: 66/100, Loss: 0.9293\n",
      "Epoch: 67/100, Loss: 0.9252\n",
      "Epoch: 68/100, Loss: 0.9306\n",
      "Epoch: 69/100, Loss: 0.9259\n",
      "Epoch: 70/100, Loss: 0.9257\n",
      "Epoch: 71/100, Loss: 0.9290\n",
      "Epoch: 72/100, Loss: 0.9257\n",
      "Epoch: 73/100, Loss: 0.9232\n",
      "Epoch: 74/100, Loss: 0.9233\n",
      "Epoch: 75/100, Loss: 0.9254\n",
      "Epoch: 76/100, Loss: 0.9256\n",
      "Epoch: 77/100, Loss: 0.9214\n",
      "Epoch: 78/100, Loss: 0.9225\n",
      "Epoch: 79/100, Loss: 0.9241\n",
      "Epoch: 80/100, Loss: 0.9226\n",
      "Epoch: 81/100, Loss: 0.9206\n",
      "Epoch: 82/100, Loss: 0.9202\n",
      "Epoch: 83/100, Loss: 0.9220\n",
      "Epoch: 84/100, Loss: 0.9187\n",
      "Epoch: 85/100, Loss: 0.9209\n",
      "Epoch: 86/100, Loss: 0.9191\n",
      "Epoch: 87/100, Loss: 0.9212\n",
      "Epoch: 88/100, Loss: 0.9178\n",
      "Epoch: 89/100, Loss: 0.9200\n",
      "Epoch: 90/100, Loss: 0.9168\n",
      "Epoch: 91/100, Loss: 0.9186\n",
      "Epoch: 92/100, Loss: 0.9179\n",
      "Epoch: 93/100, Loss: 0.9183\n",
      "Epoch: 94/100, Loss: 0.9183\n",
      "Epoch: 95/100, Loss: 0.9150\n",
      "Epoch: 96/100, Loss: 0.9149\n",
      "Epoch: 97/100, Loss: 0.9162\n",
      "Epoch: 98/100, Loss: 0.9147\n",
      "Epoch: 99/100, Loss: 0.9157\n",
      "Epoch: 100/100, Loss: 0.9140\n",
      "Evaluating...\n",
      "Test Loss: 0.9427\n",
      "0.9427406562699212\n",
      "Training...\n",
      "Epoch: 1/100, Loss: 1.9193\n",
      "Epoch: 2/100, Loss: 1.3159\n",
      "Epoch: 3/100, Loss: 1.2406\n",
      "Epoch: 4/100, Loss: 1.1880\n",
      "Epoch: 5/100, Loss: 1.1522\n",
      "Epoch: 6/100, Loss: 1.1229\n",
      "Epoch: 7/100, Loss: 1.1012\n",
      "Epoch: 8/100, Loss: 1.0854\n",
      "Epoch: 9/100, Loss: 1.0695\n",
      "Epoch: 10/100, Loss: 1.0583\n",
      "Epoch: 11/100, Loss: 1.0479\n",
      "Epoch: 12/100, Loss: 1.0374\n",
      "Epoch: 13/100, Loss: 1.0281\n",
      "Epoch: 14/100, Loss: 1.0214\n",
      "Epoch: 15/100, Loss: 1.0138\n",
      "Epoch: 16/100, Loss: 1.0106\n",
      "Epoch: 17/100, Loss: 1.0030\n",
      "Epoch: 18/100, Loss: 0.9974\n",
      "Epoch: 19/100, Loss: 0.9949\n",
      "Epoch: 20/100, Loss: 0.9889\n",
      "Epoch: 21/100, Loss: 0.9852\n",
      "Epoch: 22/100, Loss: 0.9838\n",
      "Epoch: 23/100, Loss: 0.9785\n",
      "Epoch: 24/100, Loss: 0.9768\n",
      "Epoch: 25/100, Loss: 0.9738\n",
      "Epoch: 26/100, Loss: 0.9709\n",
      "Epoch: 27/100, Loss: 0.9713\n",
      "Epoch: 28/100, Loss: 0.9659\n",
      "Epoch: 29/100, Loss: 0.9628\n",
      "Epoch: 30/100, Loss: 0.9621\n",
      "Epoch: 31/100, Loss: 0.9586\n",
      "Epoch: 32/100, Loss: 0.9587\n",
      "Epoch: 33/100, Loss: 0.9575\n",
      "Epoch: 34/100, Loss: 0.9542\n",
      "Epoch: 35/100, Loss: 0.9520\n",
      "Epoch: 36/100, Loss: 0.9528\n",
      "Epoch: 37/100, Loss: 0.9490\n",
      "Epoch: 38/100, Loss: 0.9485\n",
      "Epoch: 39/100, Loss: 0.9480\n",
      "Epoch: 40/100, Loss: 0.9447\n",
      "Epoch: 41/100, Loss: 0.9421\n",
      "Epoch: 42/100, Loss: 0.9431\n",
      "Epoch: 43/100, Loss: 0.9402\n",
      "Epoch: 44/100, Loss: 0.9404\n",
      "Epoch: 45/100, Loss: 0.9376\n",
      "Epoch: 46/100, Loss: 0.9388\n",
      "Epoch: 47/100, Loss: 0.9396\n",
      "Epoch: 48/100, Loss: 0.9366\n",
      "Epoch: 49/100, Loss: 0.9348\n",
      "Epoch: 50/100, Loss: 0.9384\n",
      "Epoch: 51/100, Loss: 0.9308\n",
      "Epoch: 52/100, Loss: 0.9354\n",
      "Epoch: 53/100, Loss: 0.9325\n",
      "Epoch: 54/100, Loss: 0.9340\n",
      "Epoch: 55/100, Loss: 0.9295\n",
      "Epoch: 56/100, Loss: 0.9289\n",
      "Epoch: 57/100, Loss: 0.9301\n",
      "Epoch: 58/100, Loss: 0.9280\n",
      "Epoch: 59/100, Loss: 0.9277\n",
      "Epoch: 60/100, Loss: 0.9305\n",
      "Epoch: 61/100, Loss: 0.9253\n",
      "Epoch: 62/100, Loss: 0.9254\n",
      "Epoch: 63/100, Loss: 0.9256\n",
      "Epoch: 64/100, Loss: 0.9250\n",
      "Epoch: 65/100, Loss: 0.9232\n",
      "Epoch: 66/100, Loss: 0.9244\n",
      "Epoch: 67/100, Loss: 0.9239\n",
      "Epoch: 68/100, Loss: 0.9230\n",
      "Epoch: 69/100, Loss: 0.9225\n",
      "Epoch: 70/100, Loss: 0.9255\n",
      "Epoch: 71/100, Loss: 0.9193\n",
      "Epoch: 72/100, Loss: 0.9220\n",
      "Epoch: 73/100, Loss: 0.9196\n",
      "Epoch: 74/100, Loss: 0.9202\n",
      "Epoch: 75/100, Loss: 0.9196\n",
      "Epoch: 76/100, Loss: 0.9206\n",
      "Epoch: 77/100, Loss: 0.9179\n",
      "Epoch: 78/100, Loss: 0.9194\n",
      "Epoch: 79/100, Loss: 0.9161\n",
      "Epoch: 80/100, Loss: 0.9171\n",
      "Epoch: 81/100, Loss: 0.9195\n",
      "Epoch: 82/100, Loss: 0.9153\n",
      "Epoch: 83/100, Loss: 0.9169\n",
      "Epoch: 84/100, Loss: 0.9141\n",
      "Epoch: 85/100, Loss: 0.9178\n",
      "Epoch: 86/100, Loss: 0.9156\n",
      "Epoch: 87/100, Loss: 0.9127\n",
      "Epoch: 88/100, Loss: 0.9177\n",
      "Epoch: 89/100, Loss: 0.9119\n",
      "Epoch: 90/100, Loss: 0.9146\n",
      "Epoch: 91/100, Loss: 0.9138\n",
      "Epoch: 92/100, Loss: 0.9138\n",
      "Epoch: 93/100, Loss: 0.9127\n",
      "Epoch: 94/100, Loss: 0.9139\n",
      "Epoch: 95/100, Loss: 0.9130\n",
      "Epoch: 96/100, Loss: 0.9100\n",
      "Epoch: 97/100, Loss: 0.9121\n",
      "Epoch: 98/100, Loss: 0.9112\n",
      "Epoch: 99/100, Loss: 0.9124\n",
      "Epoch: 100/100, Loss: 0.9095\n",
      "Evaluating...\n",
      "Test Loss: 0.9470\n",
      "0.9470094879468282\n",
      "Training...\n",
      "Epoch: 1/100, Loss: 1.9326\n",
      "Epoch: 2/100, Loss: 1.3122\n",
      "Epoch: 3/100, Loss: 1.2438\n",
      "Epoch: 4/100, Loss: 1.1952\n",
      "Epoch: 5/100, Loss: 1.1548\n",
      "Epoch: 6/100, Loss: 1.1281\n",
      "Epoch: 7/100, Loss: 1.1047\n",
      "Epoch: 8/100, Loss: 1.0888\n",
      "Epoch: 9/100, Loss: 1.0739\n",
      "Epoch: 10/100, Loss: 1.0618\n",
      "Epoch: 11/100, Loss: 1.0523\n",
      "Epoch: 12/100, Loss: 1.0425\n",
      "Epoch: 13/100, Loss: 1.0354\n",
      "Epoch: 14/100, Loss: 1.0299\n",
      "Epoch: 15/100, Loss: 1.0206\n",
      "Epoch: 16/100, Loss: 1.0165\n",
      "Epoch: 17/100, Loss: 1.0114\n",
      "Epoch: 18/100, Loss: 1.0073\n",
      "Epoch: 19/100, Loss: 1.0043\n",
      "Epoch: 20/100, Loss: 0.9977\n",
      "Epoch: 21/100, Loss: 0.9973\n",
      "Epoch: 22/100, Loss: 0.9929\n",
      "Epoch: 23/100, Loss: 0.9877\n",
      "Epoch: 24/100, Loss: 0.9841\n",
      "Epoch: 25/100, Loss: 0.9818\n",
      "Epoch: 26/100, Loss: 0.9785\n",
      "Epoch: 27/100, Loss: 0.9781\n",
      "Epoch: 28/100, Loss: 0.9728\n",
      "Epoch: 29/100, Loss: 0.9746\n",
      "Epoch: 30/100, Loss: 0.9701\n",
      "Epoch: 31/100, Loss: 0.9677\n",
      "Epoch: 32/100, Loss: 0.9664\n",
      "Epoch: 33/100, Loss: 0.9626\n",
      "Epoch: 34/100, Loss: 0.9595\n",
      "Epoch: 35/100, Loss: 0.9606\n",
      "Epoch: 36/100, Loss: 0.9590\n",
      "Epoch: 37/100, Loss: 0.9560\n",
      "Epoch: 38/100, Loss: 0.9538\n",
      "Epoch: 39/100, Loss: 0.9538\n",
      "Epoch: 40/100, Loss: 0.9523\n",
      "Epoch: 41/100, Loss: 0.9528\n",
      "Epoch: 42/100, Loss: 0.9496\n",
      "Epoch: 43/100, Loss: 0.9481\n",
      "Epoch: 44/100, Loss: 0.9454\n",
      "Epoch: 45/100, Loss: 0.9448\n",
      "Epoch: 46/100, Loss: 0.9468\n",
      "Epoch: 47/100, Loss: 0.9417\n",
      "Epoch: 48/100, Loss: 0.9428\n",
      "Epoch: 49/100, Loss: 0.9436\n",
      "Epoch: 50/100, Loss: 0.9392\n",
      "Epoch: 51/100, Loss: 0.9392\n",
      "Epoch: 52/100, Loss: 0.9395\n",
      "Epoch: 53/100, Loss: 0.9355\n",
      "Epoch: 54/100, Loss: 0.9368\n",
      "Epoch: 55/100, Loss: 0.9358\n",
      "Epoch: 56/100, Loss: 0.9350\n",
      "Epoch: 57/100, Loss: 0.9337\n",
      "Epoch: 58/100, Loss: 0.9335\n",
      "Epoch: 59/100, Loss: 0.9324\n",
      "Epoch: 60/100, Loss: 0.9328\n",
      "Epoch: 61/100, Loss: 0.9335\n",
      "Epoch: 62/100, Loss: 0.9295\n",
      "Epoch: 63/100, Loss: 0.9311\n",
      "Epoch: 64/100, Loss: 0.9271\n",
      "Epoch: 65/100, Loss: 0.9282\n",
      "Epoch: 66/100, Loss: 0.9289\n",
      "Epoch: 67/100, Loss: 0.9312\n",
      "Epoch: 68/100, Loss: 0.9274\n",
      "Epoch: 69/100, Loss: 0.9241\n",
      "Epoch: 70/100, Loss: 0.9270\n",
      "Epoch: 71/100, Loss: 0.9233\n",
      "Epoch: 72/100, Loss: 0.9267\n",
      "Epoch: 73/100, Loss: 0.9239\n",
      "Epoch: 74/100, Loss: 0.9233\n",
      "Epoch: 75/100, Loss: 0.9243\n",
      "Epoch: 76/100, Loss: 0.9235\n",
      "Epoch: 77/100, Loss: 0.9228\n",
      "Epoch: 78/100, Loss: 0.9226\n",
      "Epoch: 79/100, Loss: 0.9200\n",
      "Epoch: 80/100, Loss: 0.9202\n",
      "Epoch: 81/100, Loss: 0.9232\n",
      "Epoch: 82/100, Loss: 0.9202\n",
      "Epoch: 83/100, Loss: 0.9204\n",
      "Epoch: 84/100, Loss: 0.9188\n",
      "Epoch: 85/100, Loss: 0.9227\n",
      "Epoch: 86/100, Loss: 0.9160\n",
      "Epoch: 87/100, Loss: 0.9192\n",
      "Epoch: 88/100, Loss: 0.9169\n",
      "Epoch: 89/100, Loss: 0.9182\n",
      "Epoch: 90/100, Loss: 0.9199\n",
      "Epoch: 91/100, Loss: 0.9150\n",
      "Epoch: 92/100, Loss: 0.9144\n",
      "Epoch: 93/100, Loss: 0.9175\n",
      "Epoch: 94/100, Loss: 0.9165\n",
      "Epoch: 95/100, Loss: 0.9169\n",
      "Epoch: 96/100, Loss: 0.9139\n",
      "Epoch: 97/100, Loss: 0.9155\n",
      "Epoch: 98/100, Loss: 0.9144\n",
      "Epoch: 99/100, Loss: 0.9135\n",
      "Epoch: 100/100, Loss: 0.9181\n",
      "Evaluating...\n",
      "Test Loss: 0.9390\n",
      "0.9390481180614896\n",
      "Training...\n",
      "Epoch: 1/100, Loss: 1.9458\n",
      "Epoch: 2/100, Loss: 1.3131\n",
      "Epoch: 3/100, Loss: 1.2405\n",
      "Epoch: 4/100, Loss: 1.1941\n",
      "Epoch: 5/100, Loss: 1.1606\n",
      "Epoch: 6/100, Loss: 1.1316\n",
      "Epoch: 7/100, Loss: 1.1101\n",
      "Epoch: 8/100, Loss: 1.0909\n",
      "Epoch: 9/100, Loss: 1.0724\n",
      "Epoch: 10/100, Loss: 1.0612\n",
      "Epoch: 11/100, Loss: 1.0493\n",
      "Epoch: 12/100, Loss: 1.0392\n",
      "Epoch: 13/100, Loss: 1.0314\n",
      "Epoch: 14/100, Loss: 1.0217\n",
      "Epoch: 15/100, Loss: 1.0173\n",
      "Epoch: 16/100, Loss: 1.0099\n",
      "Epoch: 17/100, Loss: 1.0055\n",
      "Epoch: 18/100, Loss: 1.0007\n",
      "Epoch: 19/100, Loss: 0.9962\n",
      "Epoch: 20/100, Loss: 0.9931\n",
      "Epoch: 21/100, Loss: 0.9877\n",
      "Epoch: 22/100, Loss: 0.9847\n",
      "Epoch: 23/100, Loss: 0.9817\n",
      "Epoch: 24/100, Loss: 0.9784\n",
      "Epoch: 25/100, Loss: 0.9772\n",
      "Epoch: 26/100, Loss: 0.9749\n",
      "Epoch: 27/100, Loss: 0.9707\n",
      "Epoch: 28/100, Loss: 0.9709\n",
      "Epoch: 29/100, Loss: 0.9680\n",
      "Epoch: 30/100, Loss: 0.9637\n",
      "Epoch: 31/100, Loss: 0.9610\n",
      "Epoch: 32/100, Loss: 0.9593\n",
      "Epoch: 33/100, Loss: 0.9589\n",
      "Epoch: 34/100, Loss: 0.9581\n",
      "Epoch: 35/100, Loss: 0.9546\n",
      "Epoch: 36/100, Loss: 0.9546\n",
      "Epoch: 37/100, Loss: 0.9538\n",
      "Epoch: 38/100, Loss: 0.9510\n",
      "Epoch: 39/100, Loss: 0.9471\n",
      "Epoch: 40/100, Loss: 0.9491\n",
      "Epoch: 41/100, Loss: 0.9448\n",
      "Epoch: 42/100, Loss: 0.9464\n",
      "Epoch: 43/100, Loss: 0.9435\n",
      "Epoch: 44/100, Loss: 0.9426\n",
      "Epoch: 45/100, Loss: 0.9432\n",
      "Epoch: 46/100, Loss: 0.9383\n",
      "Epoch: 47/100, Loss: 0.9393\n",
      "Epoch: 48/100, Loss: 0.9389\n",
      "Epoch: 49/100, Loss: 0.9359\n",
      "Epoch: 50/100, Loss: 0.9368\n",
      "Epoch: 51/100, Loss: 0.9358\n",
      "Epoch: 52/100, Loss: 0.9356\n",
      "Epoch: 53/100, Loss: 0.9363\n",
      "Epoch: 54/100, Loss: 0.9309\n",
      "Epoch: 55/100, Loss: 0.9320\n",
      "Epoch: 56/100, Loss: 0.9320\n",
      "Epoch: 57/100, Loss: 0.9316\n",
      "Epoch: 58/100, Loss: 0.9281\n",
      "Epoch: 59/100, Loss: 0.9287\n",
      "Epoch: 60/100, Loss: 0.9297\n",
      "Epoch: 61/100, Loss: 0.9261\n",
      "Epoch: 62/100, Loss: 0.9261\n",
      "Epoch: 63/100, Loss: 0.9266\n",
      "Epoch: 64/100, Loss: 0.9264\n",
      "Epoch: 65/100, Loss: 0.9244\n",
      "Epoch: 66/100, Loss: 0.9231\n",
      "Epoch: 67/100, Loss: 0.9234\n",
      "Epoch: 68/100, Loss: 0.9225\n",
      "Epoch: 69/100, Loss: 0.9242\n",
      "Epoch: 70/100, Loss: 0.9208\n",
      "Epoch: 71/100, Loss: 0.9214\n",
      "Epoch: 72/100, Loss: 0.9217\n",
      "Epoch: 73/100, Loss: 0.9199\n",
      "Epoch: 74/100, Loss: 0.9216\n",
      "Epoch: 75/100, Loss: 0.9191\n",
      "Epoch: 76/100, Loss: 0.9191\n",
      "Epoch: 77/100, Loss: 0.9177\n",
      "Epoch: 78/100, Loss: 0.9187\n",
      "Epoch: 79/100, Loss: 0.9196\n",
      "Epoch: 80/100, Loss: 0.9179\n",
      "Epoch: 81/100, Loss: 0.9128\n",
      "Epoch: 82/100, Loss: 0.9195\n",
      "Epoch: 83/100, Loss: 0.9155\n",
      "Epoch: 84/100, Loss: 0.9139\n",
      "Epoch: 85/100, Loss: 0.9162\n",
      "Epoch: 86/100, Loss: 0.9159\n",
      "Epoch: 87/100, Loss: 0.9129\n",
      "Epoch: 88/100, Loss: 0.9155\n",
      "Epoch: 89/100, Loss: 0.9112\n",
      "Epoch: 90/100, Loss: 0.9147\n",
      "Epoch: 91/100, Loss: 0.9137\n",
      "Epoch: 92/100, Loss: 0.9134\n",
      "Epoch: 93/100, Loss: 0.9124\n",
      "Epoch: 94/100, Loss: 0.9130\n",
      "Epoch: 95/100, Loss: 0.9099\n",
      "Epoch: 96/100, Loss: 0.9099\n",
      "Epoch: 97/100, Loss: 0.9133\n",
      "Epoch: 98/100, Loss: 0.9084\n",
      "Epoch: 99/100, Loss: 0.9108\n",
      "Epoch: 100/100, Loss: 0.9102\n",
      "Evaluating...\n",
      "Test Loss: 0.9497\n",
      "0.9496841390927633\n",
      "Training...\n",
      "Epoch: 1/100, Loss: 1.9483\n",
      "Epoch: 2/100, Loss: 1.3052\n",
      "Epoch: 3/100, Loss: 1.2381\n",
      "Epoch: 4/100, Loss: 1.1926\n",
      "Epoch: 5/100, Loss: 1.1599\n",
      "Epoch: 6/100, Loss: 1.1317\n",
      "Epoch: 7/100, Loss: 1.1087\n",
      "Epoch: 8/100, Loss: 1.0898\n",
      "Epoch: 9/100, Loss: 1.0722\n",
      "Epoch: 10/100, Loss: 1.0599\n",
      "Epoch: 11/100, Loss: 1.0468\n",
      "Epoch: 12/100, Loss: 1.0364\n",
      "Epoch: 13/100, Loss: 1.0295\n",
      "Epoch: 14/100, Loss: 1.0221\n",
      "Epoch: 15/100, Loss: 1.0115\n",
      "Epoch: 16/100, Loss: 1.0091\n",
      "Epoch: 17/100, Loss: 1.0023\n",
      "Epoch: 18/100, Loss: 0.9962\n",
      "Epoch: 19/100, Loss: 0.9937\n",
      "Epoch: 20/100, Loss: 0.9901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     67\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.002\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m loss \u001b[39m=\u001b[39m train(epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m     69\u001b[0m              train_features\u001b[39m=\u001b[39mtrain_features,\n\u001b[0;32m     70\u001b[0m              train_labels\u001b[39m=\u001b[39mtrain_targets,\n\u001b[0;32m     71\u001b[0m              test_features\u001b[39m=\u001b[39mtest_features,\n\u001b[0;32m     72\u001b[0m              test_labels\u001b[39m=\u001b[39mtest_targets,\n\u001b[0;32m     73\u001b[0m              model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     74\u001b[0m              optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m     75\u001b[0m              crps\u001b[39m=\u001b[39mcrps,\n\u001b[0;32m     76\u001b[0m              device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     77\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[0;32m     78\u001b[0m test_loss_list\u001b[39m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, train_features, train_labels, test_features, test_labels, model, optimizer, crps, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mfor\u001b[39;00m batch_features, batch_labels \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m     28\u001b[0m         \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         mu, sigma \u001b[39m=\u001b[39m model(batch_features)\n\u001b[0;32m     30\u001b[0m         \u001b[39m# Compute the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\morit\\anaconda3\\envs\\GNN\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(batch, \u001b[39m0\u001b[39m, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trn_scores = []\n",
    "test_scores = []\n",
    "preds = []\n",
    "\n",
    "nreps = 10\n",
    "\n",
    "def train(epochs:int, train_features, train_labels, test_features, test_labels, model, optimizer, crps, device):\n",
    "    # Convert numpy arrays to tensors\n",
    "    train_features = torch.Tensor(train_features).to(device)\n",
    "    train_labels = torch.Tensor(train_labels).to(device)\n",
    "    test_features = torch.Tensor(test_features).to(device)\n",
    "    test_labels = torch.Tensor(test_labels).to(device)\n",
    "    # Create a TensorDataset\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    # Define the batch size\n",
    "    batch_size = 4096\n",
    "    # Create a DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_loss = 0\n",
    "    # Train\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        for batch_features, batch_labels in train_dataloader:\n",
    "            # Forward pass\n",
    "            mu, sigma = model(batch_features)\n",
    "            # Compute the loss\n",
    "            loss = crps(mu, sigma, batch_labels)\n",
    "            # Backward pass and optimization\n",
    "            # Faster Way to zero gradients\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Add Loss\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    #Eval\n",
    "    print(\"Evaluating...\")\n",
    "    with torch.inference_mode():\n",
    "        for batch_features, batch_labels in test_dataloader:\n",
    "            # Forward pass\n",
    "            mu, sigma = model(batch_features)\n",
    "            # Compute the loss\n",
    "            loss = crps(mu, sigma, batch_labels)\n",
    "            # Add Loss\n",
    "            test_loss += loss.item()\n",
    "    test_loss /= len(test_dataloader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return test_loss\n",
    "\n",
    "            \n",
    "\n",
    "test_loss_list = []\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "for e in range(nreps):\n",
    "    model = NNPP(INPUT_DIM=test_features.shape[1])\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    loss = train(epochs=100,\n",
    "                 train_features=train_features,\n",
    "                 train_labels=train_targets,\n",
    "                 test_features=test_features,\n",
    "                 test_labels=test_targets,\n",
    "                 model=model,\n",
    "                 optimizer=optimizer,\n",
    "                 crps=crps,\n",
    "                 device=device)\n",
    "    print(loss)\n",
    "    test_loss_list.append(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
