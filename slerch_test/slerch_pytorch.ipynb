{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNPP\n",
    "This Notebook reimplements the Method of the Paper from RL 18 to compare results to the pytoch geometric model.\n",
    "This should archieve a CRPS score around 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:36.075980Z",
     "start_time": "2023-07-04T22:04:36.073316Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import helpers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ...\n",
      "Cleaning Data ...\n"
     ]
    }
   ],
   "source": [
    "# read data (can be downloaded from https://doi.org/10.6084/m9.figshare.13516301.v1)\n",
    "print(\"Loading Data ...\")\n",
    "data = helpers.load_data(indexed=False)\n",
    "\n",
    "print(\"Cleaning Data ...\")\n",
    "data = helpers.clean_data(data, max_missing=121, max_alt=1000.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:45.615633Z",
     "start_time": "2023-07-04T22:04:36.080376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "                             date  station  obs  t2m_mean   t2m_var  \\\n0       2007-01-03 00:00:00+00:00        0  5.5  3.616448  0.079733   \n1       2007-01-03 00:00:00+00:00        1  2.9  4.601281  0.107129   \n2       2007-01-03 00:00:00+00:00        2  3.3  2.873910  0.078148   \n4       2007-01-03 00:00:00+00:00        4  3.4  2.718213  0.198263   \n5       2007-01-03 00:00:00+00:00        5  1.8  1.375332  0.149906   \n...                           ...      ...  ...       ...       ...   \n1808680 2016-12-31 00:00:00+00:00      530 -0.6 -0.155651  0.978417   \n1808940 2016-12-31 00:00:00+00:00      531 -5.3 -3.497557  0.172615   \n1808533 2016-12-31 00:00:00+00:00      532 -2.7 -1.661223  0.163165   \n1808941 2016-12-31 00:00:00+00:00      533 -1.5 -5.979924  0.349577   \n1808582 2016-12-31 00:00:00+00:00      534 -3.5 -4.318435  0.096924   \n\n          cape_mean     cape_var        sp_mean        sp_var   tcc_mean  ...  \\\n0         11.480126   164.398999  101263.773906  17346.641356  46.793524  ...   \n1         22.207007   207.708022  101463.529063  18411.594667  48.161629  ...   \n2         44.308516  1438.915507   97942.360781  20106.971594  63.223506  ...   \n4         96.170580  2550.754359   98045.333437  19122.337645  73.738330  ...   \n5        123.618650  1533.808489   96637.034219  16800.454370  91.842461  ...   \n...             ...          ...            ...           ...        ...  ...   \n1808680    0.201357     0.141017  102826.477656   9997.686018  49.609571  ...   \n1808940    0.003596     0.000317   96513.425000   4798.146479   0.186227  ...   \n1808533    0.021574     0.006121  102359.503125   8667.898487   7.431432  ...   \n1808941    0.000000     0.000000   95704.712813   4314.376743   1.796408  ...   \n1808582    0.089892     0.038259   97900.696563   4972.161398   1.182946  ...   \n\n              str_var    d2m_mean   d2m_var     sm_mean     sm_var        lat  \\\n0        2.810124e+11  275.956692  0.151394  318.990796   6.543392  50.782700   \n1        2.771202e+11  275.529611  0.152089  325.635452   8.776818  52.485298   \n2        4.909704e+11  275.008204  0.075718  336.861672   5.635509  50.744598   \n4        7.963386e+11  274.732042  0.186014  324.187435  13.339768  51.088100   \n5        1.033478e+12  274.021844  0.207122  315.014949  20.406193  48.405998   \n...               ...         ...       ...         ...        ...        ...   \n1808680  1.132820e+12  271.254705  1.372146  245.919016  17.199589  52.715599   \n1808940  1.159146e+11  266.253950  0.827402  395.544220  22.223388  48.441799   \n1808533  1.425973e+11  269.773757  0.461191  253.349395  13.011937  51.841801   \n1808941  6.356090e+10  263.174088  0.838257  353.039061  22.348963  47.876099   \n1808582  1.488956e+11  266.343578  0.280225  350.314387  13.186273  48.487801   \n\n             lon    alt        orog       doy  \n0         6.0941  202.0  107.439461 -0.983798  \n1         7.9126   65.0   47.632523 -0.983798  \n2         9.3450  300.0  348.869904 -0.983798  \n4        12.9326  296.0  296.839203 -0.983798  \n5        11.3117  510.0  461.575287 -0.983798  \n...          ...    ...         ...       ...  \n1808680   7.3176   19.0   36.652340 -0.973264  \n1808940   9.9216  593.0  532.696167 -0.973264  \n1808533   8.0607  104.0   95.691666 -0.973264  \n1808941  10.5848  816.0  671.567078 -0.973264  \n1808582  10.2607  444.0  507.198090 -0.973264  \n\n[1641095 rows x 44 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>station</th>\n      <th>obs</th>\n      <th>t2m_mean</th>\n      <th>t2m_var</th>\n      <th>cape_mean</th>\n      <th>cape_var</th>\n      <th>sp_mean</th>\n      <th>sp_var</th>\n      <th>tcc_mean</th>\n      <th>...</th>\n      <th>str_var</th>\n      <th>d2m_mean</th>\n      <th>d2m_var</th>\n      <th>sm_mean</th>\n      <th>sm_var</th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>alt</th>\n      <th>orog</th>\n      <th>doy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2007-01-03 00:00:00+00:00</td>\n      <td>0</td>\n      <td>5.5</td>\n      <td>3.616448</td>\n      <td>0.079733</td>\n      <td>11.480126</td>\n      <td>164.398999</td>\n      <td>101263.773906</td>\n      <td>17346.641356</td>\n      <td>46.793524</td>\n      <td>...</td>\n      <td>2.810124e+11</td>\n      <td>275.956692</td>\n      <td>0.151394</td>\n      <td>318.990796</td>\n      <td>6.543392</td>\n      <td>50.782700</td>\n      <td>6.0941</td>\n      <td>202.0</td>\n      <td>107.439461</td>\n      <td>-0.983798</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2007-01-03 00:00:00+00:00</td>\n      <td>1</td>\n      <td>2.9</td>\n      <td>4.601281</td>\n      <td>0.107129</td>\n      <td>22.207007</td>\n      <td>207.708022</td>\n      <td>101463.529063</td>\n      <td>18411.594667</td>\n      <td>48.161629</td>\n      <td>...</td>\n      <td>2.771202e+11</td>\n      <td>275.529611</td>\n      <td>0.152089</td>\n      <td>325.635452</td>\n      <td>8.776818</td>\n      <td>52.485298</td>\n      <td>7.9126</td>\n      <td>65.0</td>\n      <td>47.632523</td>\n      <td>-0.983798</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2007-01-03 00:00:00+00:00</td>\n      <td>2</td>\n      <td>3.3</td>\n      <td>2.873910</td>\n      <td>0.078148</td>\n      <td>44.308516</td>\n      <td>1438.915507</td>\n      <td>97942.360781</td>\n      <td>20106.971594</td>\n      <td>63.223506</td>\n      <td>...</td>\n      <td>4.909704e+11</td>\n      <td>275.008204</td>\n      <td>0.075718</td>\n      <td>336.861672</td>\n      <td>5.635509</td>\n      <td>50.744598</td>\n      <td>9.3450</td>\n      <td>300.0</td>\n      <td>348.869904</td>\n      <td>-0.983798</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2007-01-03 00:00:00+00:00</td>\n      <td>4</td>\n      <td>3.4</td>\n      <td>2.718213</td>\n      <td>0.198263</td>\n      <td>96.170580</td>\n      <td>2550.754359</td>\n      <td>98045.333437</td>\n      <td>19122.337645</td>\n      <td>73.738330</td>\n      <td>...</td>\n      <td>7.963386e+11</td>\n      <td>274.732042</td>\n      <td>0.186014</td>\n      <td>324.187435</td>\n      <td>13.339768</td>\n      <td>51.088100</td>\n      <td>12.9326</td>\n      <td>296.0</td>\n      <td>296.839203</td>\n      <td>-0.983798</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2007-01-03 00:00:00+00:00</td>\n      <td>5</td>\n      <td>1.8</td>\n      <td>1.375332</td>\n      <td>0.149906</td>\n      <td>123.618650</td>\n      <td>1533.808489</td>\n      <td>96637.034219</td>\n      <td>16800.454370</td>\n      <td>91.842461</td>\n      <td>...</td>\n      <td>1.033478e+12</td>\n      <td>274.021844</td>\n      <td>0.207122</td>\n      <td>315.014949</td>\n      <td>20.406193</td>\n      <td>48.405998</td>\n      <td>11.3117</td>\n      <td>510.0</td>\n      <td>461.575287</td>\n      <td>-0.983798</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1808680</th>\n      <td>2016-12-31 00:00:00+00:00</td>\n      <td>530</td>\n      <td>-0.6</td>\n      <td>-0.155651</td>\n      <td>0.978417</td>\n      <td>0.201357</td>\n      <td>0.141017</td>\n      <td>102826.477656</td>\n      <td>9997.686018</td>\n      <td>49.609571</td>\n      <td>...</td>\n      <td>1.132820e+12</td>\n      <td>271.254705</td>\n      <td>1.372146</td>\n      <td>245.919016</td>\n      <td>17.199589</td>\n      <td>52.715599</td>\n      <td>7.3176</td>\n      <td>19.0</td>\n      <td>36.652340</td>\n      <td>-0.973264</td>\n    </tr>\n    <tr>\n      <th>1808940</th>\n      <td>2016-12-31 00:00:00+00:00</td>\n      <td>531</td>\n      <td>-5.3</td>\n      <td>-3.497557</td>\n      <td>0.172615</td>\n      <td>0.003596</td>\n      <td>0.000317</td>\n      <td>96513.425000</td>\n      <td>4798.146479</td>\n      <td>0.186227</td>\n      <td>...</td>\n      <td>1.159146e+11</td>\n      <td>266.253950</td>\n      <td>0.827402</td>\n      <td>395.544220</td>\n      <td>22.223388</td>\n      <td>48.441799</td>\n      <td>9.9216</td>\n      <td>593.0</td>\n      <td>532.696167</td>\n      <td>-0.973264</td>\n    </tr>\n    <tr>\n      <th>1808533</th>\n      <td>2016-12-31 00:00:00+00:00</td>\n      <td>532</td>\n      <td>-2.7</td>\n      <td>-1.661223</td>\n      <td>0.163165</td>\n      <td>0.021574</td>\n      <td>0.006121</td>\n      <td>102359.503125</td>\n      <td>8667.898487</td>\n      <td>7.431432</td>\n      <td>...</td>\n      <td>1.425973e+11</td>\n      <td>269.773757</td>\n      <td>0.461191</td>\n      <td>253.349395</td>\n      <td>13.011937</td>\n      <td>51.841801</td>\n      <td>8.0607</td>\n      <td>104.0</td>\n      <td>95.691666</td>\n      <td>-0.973264</td>\n    </tr>\n    <tr>\n      <th>1808941</th>\n      <td>2016-12-31 00:00:00+00:00</td>\n      <td>533</td>\n      <td>-1.5</td>\n      <td>-5.979924</td>\n      <td>0.349577</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>95704.712813</td>\n      <td>4314.376743</td>\n      <td>1.796408</td>\n      <td>...</td>\n      <td>6.356090e+10</td>\n      <td>263.174088</td>\n      <td>0.838257</td>\n      <td>353.039061</td>\n      <td>22.348963</td>\n      <td>47.876099</td>\n      <td>10.5848</td>\n      <td>816.0</td>\n      <td>671.567078</td>\n      <td>-0.973264</td>\n    </tr>\n    <tr>\n      <th>1808582</th>\n      <td>2016-12-31 00:00:00+00:00</td>\n      <td>534</td>\n      <td>-3.5</td>\n      <td>-4.318435</td>\n      <td>0.096924</td>\n      <td>0.089892</td>\n      <td>0.038259</td>\n      <td>97900.696563</td>\n      <td>4972.161398</td>\n      <td>1.182946</td>\n      <td>...</td>\n      <td>1.488956e+11</td>\n      <td>266.343578</td>\n      <td>0.280225</td>\n      <td>350.314387</td>\n      <td>13.186273</td>\n      <td>48.487801</td>\n      <td>10.2607</td>\n      <td>444.0</td>\n      <td>507.198090</td>\n      <td>-0.973264</td>\n    </tr>\n  </tbody>\n</table>\n<p>1641095 rows × 44 columns</p>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:45.923413Z",
     "start_time": "2023-07-04T22:04:45.614389Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:46.048513Z",
     "start_time": "2023-07-04T22:04:46.044286Z"
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "eval_start = 1626724\n",
    "train_end = 1626723 # 2016-12-01\n",
    "\n",
    "train_features_raw = data.iloc[:train_end,3:].to_numpy()\n",
    "train_targets = data.iloc[:train_end,2].to_numpy()\n",
    "train_IDs = data.iloc[:train_end,1].to_numpy()\n",
    "\n",
    "test_features_raw = data.iloc[eval_start:,3:].to_numpy()\n",
    "test_targets = data.iloc[eval_start:,2].to_numpy()\n",
    "test_IDs = data.iloc[eval_start:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:47.147666Z",
     "start_time": "2023-07-04T22:04:46.044406Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize data\n",
    "def normalize(data, method=None, shift=None, scale=None):\n",
    "    result = np.zeros(data.shape)\n",
    "    if method == \"MAX\":\n",
    "        scale = np.max(data, axis=0)\n",
    "        shift = np.zeros(scale.shape)\n",
    "    for index in range(len(data[0])):\n",
    "        result[:,index] = (data[:,index] - shift[index]) / scale[index]\n",
    "    return result, shift, scale\n",
    "\n",
    "train_features, train_shift, train_scale = normalize(train_features_raw[:,:], method=\"MAX\")\n",
    "\n",
    "test_features = normalize(test_features_raw[:,:], shift=train_shift, scale=train_scale)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Newtork"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:47.147855Z",
     "start_time": "2023-07-04T22:04:47.145965Z"
    }
   },
   "outputs": [],
   "source": [
    "def crps(mu_sigma: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Calculates the Continuous Ranked Probability Score (CRPS) assuming normally distributed df\n",
    "\n",
    "    :param torch.Tensor mu_sigma: tensor of mean and standard deviation\n",
    "    :param torch.Tensor y: observed df\n",
    "\n",
    "    :return tensor: CRPS value\n",
    "    :rtype torch.Tensor\n",
    "    \"\"\"\n",
    "    mu, sigma = torch.split(mu_sigma, 1, dim=-1)\n",
    "    y = y.view((-1,1)) # make sure y has the right shape\n",
    "    pi = np.pi #3.14159265359\n",
    "    omega = (y - mu) / sigma\n",
    "    # PDF of normal distribution at omega\n",
    "    pdf = 1/(torch.sqrt(torch.tensor(2 * pi))) * torch.exp(-0.5 * omega ** 2)\n",
    "\n",
    "    # Source: https://stats.stackexchange.com/questions/187828/how-are-the-error-function-and-standard-normal-distribution-function-related\n",
    "    cdf = 0.5 * (1 + torch.erf(omega / torch.sqrt(torch.tensor(2))))\n",
    "\n",
    "    crps_score = sigma * (omega * (2 * cdf - 1) + 2 * pdf - 1/torch.sqrt(torch.tensor(pi)))\n",
    "    return  torch.mean(crps_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:47.169842Z",
     "start_time": "2023-07-04T22:04:47.146208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class NNPP(nn.Module):\n",
    "    def __init__(self, INPUT_DIM:int):\n",
    "        super(NNPP, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=535, embedding_dim=2)\n",
    "        self.lin1 = nn.Linear(in_features=INPUT_DIM+2, out_features=512)\n",
    "        self.lin2 = nn.Linear(in_features=512, out_features=2)\n",
    "\n",
    "    def forward(self, x, id):\n",
    "        emb_station = self.emb(id)\n",
    "        x = torch.cat((emb_station, x), dim=1) # Concatenate embedded station_id to rest of the feature vector\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:12:28.266013Z",
     "start_time": "2023-07-04T22:04:47.169555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [07:14<00:00, 28.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090720415115356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 86\u001B[0m\n\u001B[1;32m     84\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     85\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.002\u001B[39m)\n\u001B[0;32m---> 86\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m             \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m             \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n\u001B[1;32m     90\u001B[0m model_list\u001B[38;5;241m.\u001B[39mappend(model)\n",
      "Cell \u001B[0;32mIn[48], line 44\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(epochs, model, optimizer)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m trange(epochs):\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x, \u001B[38;5;28mid\u001B[39m, y \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m     45\u001B[0m         \u001B[38;5;66;03m# Zero gradients\u001B[39;00m\n\u001B[1;32m     46\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m--> 119\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m    122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[0;32m~/.conda/envs/GNN2/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    160\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    161\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[0;32m--> 162\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Convert numpy arrays to tensors and put it in GPU memory for faster training\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
    "train_targets = torch.tensor(train_targets, dtype=torch.float32).to(device)\n",
    "train_IDs = torch.tensor(train_IDs, dtype=torch.int32).to(device)\n",
    "\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "test_targets = torch.tensor(test_targets, dtype=torch.float32).to(device)\n",
    "test_IDs = torch.tensor(test_IDs, dtype=torch.int32).to(device)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 4096\n",
    "# Create a DataLoader\n",
    "class DatasetWithID(Dataset):\n",
    "    def __init__(self, train_target, train_id, train_feature):\n",
    "        self.train_target = train_target\n",
    "        self.train_id = train_id\n",
    "        self.train_features = train_feature\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_target)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.train_features[index]\n",
    "        id = self.train_id[index]\n",
    "        y = self.train_target[index]\n",
    "        return x, id, y\n",
    "\n",
    "train_dataset = DatasetWithID(train_targets, train_IDs, train_features)\n",
    "test_dataset = DatasetWithID(test_targets, test_IDs, test_features)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def train(epochs:int, model, optimizer):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    test_loss = 0\n",
    "    # Train\n",
    "    for epoch in trange(epochs):\n",
    "        for x, id, y in train_loader:\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            mu_sigma = model(x, id)\n",
    "            # Compute the loss\n",
    "            loss = crps(mu_sigma, y)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Add Loss\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(train_loader)\n",
    "\n",
    "    #Eval\n",
    "    with torch.inference_mode():\n",
    "        for x, id, y in test_loader:\n",
    "            # Forward pass\n",
    "            mu_sigma = model(x, id)\n",
    "            # Compute the loss\n",
    "            loss = crps(mu_sigma, y)\n",
    "            # Add Loss\n",
    "            test_loss += loss.item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "# Train Ensemble\n",
    "model_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "trn_scores = []\n",
    "test_scores = []\n",
    "preds = []\n",
    "\n",
    "nreps = 10\n",
    "\n",
    "\n",
    "for e in range(nreps):\n",
    "    model = NNPP(INPUT_DIM=test_features.shape[1])\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    loss = train(epochs=15,\n",
    "                 model=model,\n",
    "                 optimizer=optimizer)\n",
    "    print(loss)\n",
    "    model_list.append(model)\n",
    "    test_loss_list.append(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
