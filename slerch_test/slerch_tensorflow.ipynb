{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNPP\n",
    "This Notebook reimplements the Method of the Paper from RL 18 to compare results to the pytoch geometric model.\n",
    "This should archieve a CRPS score around 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:36.075980Z",
     "start_time": "2023-07-04T22:04:36.073316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(indexed: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the data from the specified file and preprocess it.\n",
    "\n",
    "    :param indexed: Whether to add a DateTimeIndex to the DataFrame. Defaults to True.\n",
    "    :type indexed: bool, optional\n",
    "    :return: The preprocessed DataFrame.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    df = pd.read_feather(\"/Users/moritzfeik/Developer/BA/data_RL18.feather\")\n",
    "    # convert station to integer and subtract 1 to make it 0-based\n",
    "    df.station = pd.to_numeric(df.station, downcast='integer') - 1\n",
    "    df = df.sort_values(by=['date', 'station'])  # sort by date and station\n",
    "    df[\"doy\"] = df[\"date\"].apply(lambda x: math.sin(((x.day_of_year-105)/366)*2*math.pi))  # Sin transformed day of year\n",
    "    if indexed:\n",
    "        df.index = df.date  # add DatetimeIndex\n",
    "        df.index = df.index.tz_convert(None)  # remove timezone\n",
    "    return df\n",
    "\n",
    "def clean_data(df: pd.DataFrame, max_missing: int = 121, max_alt: float = 1000.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame by removing outliers and stations with a high number of missing values.\n",
    "\n",
    "    :param df: The DataFrame to be cleaned.\n",
    "    :type df: pd.DataFrame\n",
    "    :param max_missing: The maximum number of rows with missing values allowed for each station. Defaults to 121.\n",
    "    :type max_missing: int, optional\n",
    "    :param max_alt: The maximum altitude of stations to keep. Stations with altitudes above this value will be dropped. Defaults to 1000.0.\n",
    "    :type max_alt: float, optional\n",
    "    :return: The cleaned DataFrame.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # drop stations with altitude > max_alt\n",
    "    df = df[df['alt'] < max_alt]\n",
    "    # drop stations with more than max_missing missing values completely\n",
    "    stations_missing_data = df.station[df.sm_mean.isna()].to_numpy()\n",
    "    stations_missing_data, counts = np.unique(stations_missing_data, return_counts=True)\n",
    "    stations_to_drop = stations_missing_data[counts > max_missing]\n",
    "    df = df[~df['station'].isin(stations_to_drop)]\n",
    "    # drop all rows with missing values\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:45.615633Z",
     "start_time": "2023-07-04T22:04:36.080376Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ...\n",
      "Cleaning Data ...\n"
     ]
    }
   ],
   "source": [
    "# read data (can be downloaded from https://doi.org/10.6084/m9.figshare.13516301.v1)\n",
    "print(\"Loading Data ...\")\n",
    "data = load_data(indexed=False)\n",
    "\n",
    "print(\"Cleaning Data ...\")\n",
    "data = clean_data(data, max_missing=121, max_alt=1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T22:04:45.923413Z",
     "start_time": "2023-07-04T22:04:45.614389Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469770"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index of last day in 2015\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "idx = max(data[data.date.dt.year == 2015].index)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "eval_start = idx+1\n",
    "train_end = idx # 2016-12-01\n",
    "\n",
    "train_features_raw = data.iloc[:train_end,3:].to_numpy()\n",
    "train_targets = data.iloc[:train_end,2].to_numpy()\n",
    "train_IDs = data.iloc[:train_end,1].to_numpy()\n",
    "\n",
    "test_features_raw = data.iloc[eval_start:,3:].to_numpy()\n",
    "test_targets = data.iloc[eval_start:,2].to_numpy()\n",
    "test_IDs = data.iloc[eval_start:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# normalize data\n",
    "def normalize(data, method=None, shift=None, scale=None):\n",
    "    result = np.zeros(data.shape)\n",
    "    if method == \"MAX\":\n",
    "        scale = np.max(data, axis=0)\n",
    "        shift = np.zeros(scale.shape)\n",
    "    for index in range(len(data[0])):\n",
    "        result[:,index] = (data[:,index] - shift[index]) / scale[index]\n",
    "    return result, shift, scale\n",
    "\n",
    "train_features, train_shift, train_scale = normalize(train_features_raw[:,:], method=\"MAX\")\n",
    "\n",
    "test_features = normalize(test_features_raw[:,:], shift=train_shift, scale=train_scale)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam  # better for M1/M2 Mac\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crps_cost_function(y_true, y_pred, theano=False):\n",
    "    \"\"\"Compute the CRPS cost function for a normal distribution defined by\n",
    "    the mean and standard deviation.\n",
    "\n",
    "    Code inspired by Kai Polsterer (HITS).\n",
    "\n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Tensor containing predictions: [mean, std]\n",
    "        theano: Set to true if using this with pure theano.\n",
    "\n",
    "    Returns:\n",
    "        mean_crps: Scalar with mean CRPS over batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Split input\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma = y_pred[:, 1]\n",
    "    # Ugly workaround for different tensor allocation in keras and theano\n",
    "    if not theano:\n",
    "        y_true = y_true[:, 0]   # Need to also get rid of axis 1 to match!\n",
    "\n",
    "    # To stop sigma from becoming negative we first have to\n",
    "    # convert it the the variance and then take the square\n",
    "    # root again.\n",
    "    var = sigma ** 2\n",
    "    # The following three variables are just for convenience\n",
    "    loc = (y_true - mu) / tf.sqrt(var)\n",
    "    phi = 1.0 / tf.sqrt(2.0 * np.pi) * tf.exp(-loc ** 2 / 2.0)\n",
    "    Phi = 0.5 * (1.0 + tf.math.erf(loc / tf.sqrt(2.0)))\n",
    "    # First we will compute the crps for each input/target pair\n",
    "    crps =  tf.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / tf.sqrt(np.pi))\n",
    "    # Then we take the mean. The cost is now a scalar\n",
    "    return tf.math.reduce_mean(crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:30<00:00, 27.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# training multiple models in a loop\n",
    "\n",
    "emb_size = 2\n",
    "max_id = int(tf.math.reduce_max([train_IDs.max(), test_IDs.max()]))\n",
    "n_features = train_features.shape[1]\n",
    "n_outputs = 2\n",
    "\n",
    "nreps = 10\n",
    "trn_scores = []\n",
    "test_scores = []\n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(nreps)):\n",
    "    clear_session()\n",
    "\n",
    "    features_in = Input(shape=(n_features,))\n",
    "    id_in = Input(shape=(1,))\n",
    "    emb = Embedding(max_id + 1, emb_size)(id_in)\n",
    "    emb = Flatten()(emb)\n",
    "    x = Concatenate()([features_in, emb])\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(n_outputs, activation='linear')(x)\n",
    "    nn_aux_emb = Model(inputs=[features_in, id_in], outputs=x)\n",
    "\n",
    "    opt = Adam(learning_rate=0.002)\n",
    "    nn_aux_emb.compile(optimizer=opt, loss=crps_cost_function)\n",
    "\n",
    "    nn_aux_emb.fit([train_features, train_IDs], train_targets, epochs=15, batch_size=4096, verbose=0)\n",
    "\n",
    "    trn_scores.append(nn_aux_emb.evaluate([train_features, train_IDs], train_targets, 4096, verbose=0))\n",
    "    test_scores.append(nn_aux_emb.evaluate([test_features, test_IDs], test_targets, 4096, verbose=0))\n",
    "    preds.append(nn_aux_emb.predict([test_features, test_IDs], 4096, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7906690835952759,\n",
       " 0.794194757938385,\n",
       " 0.7989064455032349,\n",
       " 0.7906081676483154,\n",
       " 0.7908093929290771,\n",
       " 0.7993164658546448,\n",
       " 0.8161780834197998,\n",
       " 0.7906683087348938,\n",
       " 0.7885107398033142,\n",
       " 0.7953349947929382]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble test score = 0.7865278714841577\n"
     ]
    }
   ],
   "source": [
    "# evaluate ensemble of models\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def crps_normal(mu, sigma, y):\n",
    "    \"\"\"\n",
    "    Compute CRPS for a Gaussian distribution.\n",
    "    \"\"\"\n",
    "    # Make sure sigma is positive\n",
    "    sigma = np.abs(sigma)\n",
    "    loc = (y - mu) / sigma\n",
    "    crps = sigma * (loc * (2 * norm.cdf(loc) - 1) +\n",
    "                    2 * norm.pdf(loc) - 1. / np.sqrt(np.pi))\n",
    "    return crps\n",
    "\n",
    "preds = np.array(preds)\n",
    "preds[:, :, 1] = np.abs(preds[:, :, 1]) # Make sure std is positive\n",
    "mean_preds = np.mean(preds, 0)\n",
    "ens_score = crps_normal(mean_preds[:, 0], mean_preds[:, 1], test_targets).mean()\n",
    "print(f'Ensemble test score = {ens_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
